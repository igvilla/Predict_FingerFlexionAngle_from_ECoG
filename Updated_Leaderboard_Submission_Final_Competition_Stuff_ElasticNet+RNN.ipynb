{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igvilla/Predict_FingerFlexionAngle_from_ECoG/blob/main/Updated_Leaderboard_Submission_Final_Competition_Stuff_ElasticNet%2BRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQb8UNexzK6U"
      },
      "source": [
        "# BE 521: Final Project Part 1\n",
        "\n",
        "Collaborators (Group Members): Anusha Keshireddy, Jennifer Luo, Ignacio Villasmil \n",
        "\n",
        "Spring 2022\n",
        "\n",
        "Adapted by Kevin Xie\n",
        "\n",
        "32 Points\n",
        "\n",
        "Objective: Predict finger movements from ECoG Recordings\n",
        "\n",
        "Due: March 31st\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLzcML-x6jnB"
      },
      "source": [
        "# 1. Getting Started (4 pts)\n",
        "The following sections will walk you through the development of the prediction pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w4AGrXqUt5Q"
      },
      "outputs": [],
      "source": [
        "#Set up the notebook environment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from scipy.stats import pearsonr\n",
        "from scipy import signal as sig\n",
        "\n",
        "import pdb #for breakpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA2hB_8a_Lu3"
      },
      "source": [
        "## 1.\n",
        "Extract the dataglove and ECoG data for each subject from the pickle file. Feel free to copy the code snippet above. Split the data into a training and testing set (at least 50% of the data should be in the training set). \n",
        "\n",
        "**How many samples are there in the full ECoG recording (before splitting)?** (1 pt)\n",
        "\n",
        "**How many samples do you have in your training set? In your testing set?** (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kBEB4asldFa",
        "outputId": "18cf584a-ca36-4ed9-b8dd-ba4630b08d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Spring 2022/BE521/Final\n"
          ]
        }
      ],
      "source": [
        "#for Jennifer\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #get drive\n",
        "\n",
        "#change depending on person? \n",
        "%cd /content/drive/MyDrive/Spring 2022/BE521/Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8awSNAAJvN_V",
        "outputId": "8d04bc97-e838-4d34-a2c2-63f60bd589d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/1z5dynlKdNP0PkOgS8xVONN9OwfUOLKIU/Final\n"
          ]
        }
      ],
      "source": [
        "# for Iggy:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #get drive\n",
        "\n",
        "#change depending on person? \n",
        "%cd /content/drive/MyDrive/BE521/Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3kUBgoByHHB",
        "outputId": "a1e1f0b0-4ecf-43c4-cef1-2e57d1527992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1z5dynlKdNP0PkOgS8xVONN9OwfUOLKIU/Final\n"
          ]
        }
      ],
      "source": [
        "# for Anusha:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #get drive\n",
        "\n",
        "#change depending on person? \n",
        "%cd /content/drive/MyDrive/Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvN0tbGU3Pjv"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "proj_data=sio.loadmat('raw_training_data.mat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQhxWQ6DH2W7"
      },
      "outputs": [],
      "source": [
        "#splitting into testing and training sets\n",
        "\n",
        "#patient 1\n",
        "\n",
        "#glove\n",
        "total_p1_glove=proj_data['train_dg'][0][0]\n",
        "training_p1_glove=total_p1_glove[0:int(300000*0.6),:] #60% of samples in training\n",
        "testing_p1_glove=total_p1_glove[int(300000*0.6):300000,:] #remaining 40% in testing\n",
        "#ecog\n",
        "total_p1_ecog=proj_data['train_ecog'][0][0]\n",
        "training_p1_ecog=total_p1_ecog[0:int(300000*0.6),np.arange(62)!=54] #60% of samples in training\n",
        "testing_p1_ecog=total_p1_ecog[int(300000*0.6):300000,np.arange(62)!=54] #remaining 40% in testing\n",
        "\n",
        "#patient 2\n",
        "\n",
        "#glove\n",
        "total_p2_glove=proj_data['train_dg'][1][0]\n",
        "training_p2_glove=total_p2_glove[0:int(300000*0.6),:] #60% of samples in training\n",
        "testing_p2_glove=total_p2_glove[int(300000*0.6):300000,:] #remaining 40% in testing\n",
        "#ecog\n",
        "total_p2_ecog=proj_data['train_ecog'][1][0]\n",
        "training_p2_ecog=total_p2_ecog[0:int(300000*0.6),np.logical_and(np.arange(48)!=20,np.arange(48)!=37)] #60% of samples in training\n",
        "testing_p2_ecog=total_p2_ecog[int(300000*0.6):300000,np.logical_and(np.arange(48)!=20,np.arange(48)!=37)] #remaining 40% in testing\n",
        "\n",
        "#patient 3\n",
        "\n",
        "#glove\n",
        "total_p3_glove=proj_data['train_dg'][2][0]\n",
        "training_p3_glove=total_p3_glove[0:int(300000*0.6),:] #60% of samples in training\n",
        "testing_p3_glove=total_p3_glove[int(300000*0.6):300000,:] #remaining 40% in testing\n",
        "#ecog\n",
        "total_p3_ecog=proj_data['train_ecog'][2][0]\n",
        "training_p3_ecog=total_p3_ecog[0:int(300000*0.6),:] #60% of samples in training\n",
        "testing_p3_ecog=total_p3_ecog[int(300000*0.6):300000,:] #remaining 40% in testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEoLhYHT_2Qr"
      },
      "source": [
        "There are 300,000 samples in the full ECOG recording per patient before splitting into testing and training sets. 60% of the samples were put into training set, and 40% in testing set. This corresponded to 180,000 training samples per patient, and 120,000 testing samples per patient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ9Nl70hkHBa"
      },
      "source": [
        "## 2.\n",
        "Next, complete the `filter_data` function. Test it using the raw data extracted in the prior step. What filter types and cutoff frequencies did you use? (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqfertGZGuYQ"
      },
      "outputs": [],
      "source": [
        "def filter_data(raw_eeg, srate=1000, cutoffs=[75, 200]):\n",
        "  \"\"\"\n",
        "  Write a filter function to clean underlying data.\n",
        "  Filter type and parameters are up to you. Points will be awarded for reasonable filter type, parameters and application.\n",
        "  Please note there are many acceptable answers, but make sure you aren't throwing out crucial data or adversly\n",
        "  distorting the underlying data!\n",
        "\n",
        "  Input: \n",
        "    raw_eeg (samples x channels): the raw signal\n",
        "    fs: the sampling rate (1000 for this dataset)\n",
        "  Output: \n",
        "    clean_data (samples x channels): the filtered signal\n",
        "  \"\"\"\n",
        "\n",
        "  numChan = np.shape(raw_eeg)[1] #number of channels\n",
        "  filteredData = np.zeros(np.shape(raw_eeg)); #filtered data output\n",
        "\n",
        "  #butterworth filter of 4th order\n",
        "  sos = sig.butter(4, cutoffs, 'bandpass', analog=False, fs=srate, output='sos'); # returns filter coefficients\n",
        "\n",
        "  #for each channel\n",
        "  for chanInd in np.arange(numChan):\n",
        "    #mean subtract data\n",
        "    currFilt = raw_eeg[:, chanInd] - np.mean(raw_eeg[:, chanInd]);\n",
        "    #filter between 075 and 115 hz with a filtfilt (no phase distortion)\n",
        "    currFilt = sig.sosfiltfilt(sos, currFilt) # forward-backward digital filter using cascaded second-order sections\n",
        "                                              # sos = filter coefficients ... currFilt = data to be filtered\n",
        "    filteredData[:, chanInd] = currFilt\n",
        "  return filteredData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T3tW5XUiw73"
      },
      "source": [
        "According to Kubanek 2009, frequencies above 50 Hz are relevant to finger movement decoding. When a fast-fourier transform of the data was computed, there was a high power at 60 Hz, which likely corresponds to electrical noise. Thus, 50-75 Hz was avoided. A 4th order butterworth bandpass filter with cutoff frequencies 75 and 200Hz was used. The filter was run fowards and backwards over the signal with filtfilt to remove phase distortion. \n",
        "\n",
        "The cutoff frequencies of 75-115 Hz were chosen to isolate the high gamma band, which showed significant activity changes according to Kubanek 2009. Therefore, while an general cutoff was initialized to be 75-200 Hz, an additional argument was added in the case that other frequency bands had useful features, including the gamma band. \n",
        "\n",
        "It is also important to note that there is a resonant frequency of 60 Hz noise at 120 Hz, so this frequency should be avoided when extracting features in relevant frequency bands. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBLGuLZaFGJu"
      },
      "source": [
        "**The following section is us exploring the data. **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1ZfISDvuuzI"
      },
      "source": [
        "# 2. Calculating Features (12 points)\n",
        "\n",
        "Here you will complete the `get_windowed_feats` and `get_features` functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL3CMDPovL3l"
      },
      "source": [
        "## 1. \n",
        "We will calculate features across sliding time windows. if we use a suggested window length of 100ms with a 50ms window overlap, how many feature windows, $M$, will we have if we computed features using all the data in a given subject? Feel free to re-use code from previous homeworks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc5y0fcb__N3"
      },
      "outputs": [],
      "source": [
        "#this function returns the number of windows and the remainder time (outside window)\n",
        "\n",
        "def NumWins(x,fs,winLen,winDisp):\n",
        "  #all times in seconds\n",
        "\n",
        "  #calculate number of samples\n",
        "  xLen=len(x)\n",
        "\n",
        "  #calculate length of signal in time (seconds)\n",
        "  xtime=xLen/fs\n",
        "\n",
        "  #calculate winDisp and winLen in samples\n",
        "  winLen_samples=round(winLen*fs) #window length in samples\n",
        "  winDisp_samples=round(winDisp*fs) #window displacement in samples\n",
        "\n",
        "  #calculate number of windows and the remainder of samples left over. \n",
        "\n",
        "  #if not overlapping\n",
        "  #if winDisp==winLen:\n",
        "    #NumWins=xtime//winDisp #when not overlapping, the length of the window=window displacement. Divide by total signal by this. \n",
        "    #rem_windows=xtime-(NumWins*winDisp) #answer given in time\n",
        "\n",
        "  #if there is overlap \n",
        "  #if winLen>winDisp:\n",
        "    #NumWins=(xtime-(winLen-winDisp))//winDisp #when overlapping, remainder+(windowlength-displacement)+displacement*num_windows=length of signal\n",
        "    #rem_windows=xtime-(winLen-winDisp)-(NumWins*winDisp) #answer given in time\n",
        "  \n",
        "  #if not overlapping\n",
        "  if winDisp_samples==winLen_samples:\n",
        "    NumWins=xLen//winDisp_samples #when not overlapping, the length of the window=window displacement. Divide by total signal by this. \n",
        "    rem_windows=xLen-(NumWins*winDisp_samples) #answer given in samples\n",
        "\n",
        "  #if there is overlap \n",
        "  if winLen_samples>winDisp_samples:\n",
        "    NumWins=(xLen-(winLen_samples-winDisp_samples))//winDisp_samples #when overlapping, remainder+(windowlength-displacement)+displacement*num_windows=length of signal\n",
        "    rem_windows=xLen-(winLen_samples-winDisp_samples)-(NumWins*winDisp_samples) #answer given in samples\n",
        "\n",
        "  return [NumWins,rem_windows]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUFxdvR8AAMk"
      },
      "source": [
        "There are 5998 feature windows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBBgRW2WwMFN"
      },
      "source": [
        "## 2.\n",
        "Now complete the `get_features` function. Please create **4 or more** different features to calculate for each channel in each time window. Features may include the average time-domain voltage, or the average frequency-domain magnitude in consecutive 15Hz frequency bands, bandpower of relevant frequency bands, etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyiEJArny_rw"
      },
      "outputs": [],
      "source": [
        "def time_avg(signal):\n",
        "  return np.mean(signal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40khJp9G2F1I"
      },
      "outputs": [],
      "source": [
        "from numpy.fft import fft, ifft\n",
        "\n",
        "def band_power(signal,fs,low,high):\n",
        "  X = fft(signal)\n",
        "  N = len(X)\n",
        "  n = np.arange(N)\n",
        "  T = N/fs #sampling rate=1000\n",
        "  freq = n/T \n",
        "  power_spectrum=np.abs(X)\n",
        "\n",
        "  #to get average bandpower, find frequencies between a certain range and integrate between them\n",
        "\n",
        "  # Find values in frequency vector corresponding to gamma band\n",
        "  index_band = np.logical_and(freq >= low, freq <= high)\n",
        "  from scipy.integrate import simps\n",
        "\n",
        "  # Frequency resolution\n",
        "  #freq_res = freq[1] - freq[0]  #diff between frequency values\n",
        "\n",
        "  # Compute the absolute power by approximating the area under the curve\n",
        "  #band_power = simps(power_spectrum[index_band], dx=freq_res)\n",
        "\n",
        "  #average frequency domain magnitude\n",
        "  avg_freq_domain_mag=np.mean(power_spectrum[index_band])\n",
        "\n",
        "  return avg_freq_domain_mag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxJodFLuzeL1"
      },
      "source": [
        "Possible new features to try: -Take the filtered window and filter again- specifically with the gamma band (75-115 Hz). -----calculate time domain magnitude in this band -----calculate frequency domain magnitude in this band\n",
        "\n",
        "-repeat for 125-160 Hz, 160-200 Hz.\n",
        "\n",
        "These features may be useful instead of energy, line length, and zero crossings, which may not be very useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KNhkCWA2CWY"
      },
      "outputs": [],
      "source": [
        "def get_features(filtered_window, fs=1000):\n",
        "  \"\"\"\n",
        "    Write a function that calculates features for a given filtered window. \n",
        "    Feel free to use features you have seen before in this class, features that\n",
        "    have been used in the literature, or design your own!\n",
        "\n",
        "    Input: \n",
        "      filtered_window (window_samples x channels): the window of the filtered ecog signal \n",
        "      fs: sampling rate\n",
        "    Output:\n",
        "      features (channels x num_features): the features calculated on each channel for the window\n",
        "  \"\"\"\n",
        "\n",
        "  [window_samples,num_channels]=np.shape(filtered_window)\n",
        "\n",
        "  #initialize features array, shape=(num_channels,num_features). \n",
        "  #tells us the features for a filtered window (all channels of that window)\n",
        "\n",
        "  #for now, I'm assuming I have six features, I will add more later\n",
        "\n",
        "  features=np.empty((num_channels,7))\n",
        "\n",
        "  for cc in range(num_channels):\n",
        "\n",
        "    #get the window for each channel\n",
        "    curr_window=filtered_window[:,cc] \n",
        "\n",
        "    #calculate the features for that channel, window and add to matrix\n",
        "    #add additional features later!!!!\n",
        "\n",
        "    freq_dom_mag_8_12=band_power(curr_window,1000,8,12)\n",
        "    #freq_dom_mag_20_25=band_power(curr_window,1000,20,25)\n",
        "    low_gamma_freq_dom_mag=band_power(curr_window,1000,75,95)\n",
        "    high_gamma_freq_dom_mag=band_power(curr_window,1000,96,115)\n",
        "    freq_dom_mag_146_160=band_power(curr_window,1000,146,160)\n",
        "    freq_dom_mag_125_145=band_power(curr_window,1000,125,145)\n",
        "    freq_dom_mag_160_200=band_power(curr_window,1000,160,175)\n",
        "\n",
        "\n",
        "    features[cc,0]=time_avg(curr_window)\n",
        "    features[cc,6]=freq_dom_mag_8_12\n",
        "    features[cc,1]=low_gamma_freq_dom_mag\n",
        "    features[cc,2]=freq_dom_mag_125_145\n",
        "    features[cc,3]=freq_dom_mag_160_200\n",
        "    features[cc,4]=high_gamma_freq_dom_mag\n",
        "    features[cc,5]=freq_dom_mag_146_160\n",
        "    \n",
        "\n",
        "  \n",
        "  return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPbDhVki7MnS"
      },
      "source": [
        "## 3.\n",
        "Now finish the `get_windowed_feats` function by putting the `filter_data` and `get_features` functions together to return a feature vector for each time window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STXDXJz27qZA"
      },
      "outputs": [],
      "source": [
        "#all times in seconds\n",
        "def get_windowed_feats(raw_ecog, fs, window_length, window_overlap):\n",
        "\n",
        "  clean_data=filter_data(raw_ecog, fs) #outputs filtered data (samples x channels)\n",
        "  [num_samples,num_channels]=np.shape(clean_data)\n",
        "\n",
        "  #split the filtered data into windows- for every single channel. For each window, calculate features. \n",
        "  \n",
        "  #for numWins function- need to input only one channel of data\n",
        "  x=clean_data[:,0] #takes the first channel\n",
        "\n",
        "  [numWindows,rem_samples]=NumWins(x,fs,window_length,window_overlap) #calculate number of windows and remaining samples\n",
        "\n",
        "  #convert everything to units of samples\n",
        "  winLen_samples=round(window_length*fs) #window length in samples\n",
        "  winDisp_samples=round(window_overlap*fs) #window displacement in samples \n",
        "\n",
        "  #initialize empty feature vector- length of number of windows\n",
        "  all_feats=np.zeros([round(numWindows),num_channels*7]) \n",
        "\n",
        "  #starting index of x- first window occurs after remainder samples since right aligned\n",
        "  window_start=rem_samples\n",
        "\n",
        "  for ww in range(round(numWindows)): #for each window\n",
        "    window_end=window_start+winLen_samples\n",
        "    filtered_window=clean_data[window_start:window_end,:] #signal in that window, all channels\n",
        "    \n",
        "    #find features of that window for all channels\n",
        "    features=get_features(filtered_window, fs=1000) \n",
        "\n",
        "    #add collapsed feature array into the allfeats array\n",
        "    #we will do feature 1, all channels, then feature 2, all channels, etc.\n",
        "    all_feats[ww,:]=features.flatten('F') #flattens by column so get the structure we want\n",
        "\n",
        "    #set new starting index of window\n",
        "    window_start=window_start+winDisp_samples #prior start + window displacement\n",
        "  \n",
        "  return all_feats\n",
        "\n",
        "  \"\"\"\n",
        "    Write a function which processes data through the steps of filtering and\n",
        "    feature calculation and returns features. Points will be awarded for completing\n",
        "    each step appropriately (note that if one of the functions you call within this script\n",
        "    returns a bad output, you won't be double penalized). Note that you will need\n",
        "    to run the filter_data and get_features functions within this function. \n",
        "\n",
        "    Inputs:\n",
        "      raw_eeg (samples x channels): the raw signal\n",
        "      fs: the sampling rate (1000 for this dataset)\n",
        "      window_length: the window's length\n",
        "      window_overlap: the window's overlap\n",
        "    Output: \n",
        "      all_feats (num_windows x (channels x features)): the features for each channel for each time window\n",
        "        note that this is a 2D array. \n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tNcrgHN5A8G"
      },
      "source": [
        "# 3. Creating the Response Matrix (6 points)\n",
        "In this section, you will develop code for your *create_R_matrix* function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDwxvZ7k5YNv"
      },
      "source": [
        "## 2. \n",
        "We do not have feature data to fill out the first N-1 data rows in the R matrix that will be used to predict the first N-1 finger angles. One way to work around this is to append a copy of the first N-1 rows of your feature matrix to the beginning of your feature matrix before calculating R. Make this adjustment in `create_R_matrix`, then compute the response matrix R. You can test whether your function is running correctly by running `create_R_matrix` with data from `testRfunction.pkl` using 3 windows and verifying that the quantity `np.mean(R)` is 25.4668 (5 points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2KnS-KypX_r"
      },
      "outputs": [],
      "source": [
        "def create_R_matrix(features, N_wind):\n",
        "  \"\"\" \n",
        "  Write a function to calculate the R matrix\n",
        "\n",
        "  Input:\n",
        "    features (samples (number of windows in the signal) x channels x features): \n",
        "      the features you calculated using get_windowed_feats\n",
        "    N_wind: number of windows to use in the R matrix\n",
        "\n",
        "  Output:\n",
        "    R (samples x (N_wind*channels*features))\n",
        "  \"\"\"\n",
        "  # first, add N-1 rows to the features matrix:\n",
        "  \n",
        "  # featMat[0] = first row\n",
        "  # featMat[1] = second row\n",
        "\n",
        "  new_firstRow = features[0]\n",
        "  new_secondRow = features[1]\n",
        "\n",
        "  features_appended = np.vstack([new_secondRow, features])  # adding second row to the top of the matrix first --> assigning it to a new variable\n",
        "  features_appended = np.vstack([new_firstRow, features_appended])  # adding first row to the top of the NEW matrix now (ontop of the added second row)\n",
        "\n",
        "  # INITIALIZE EMPTY R MATRIX:\n",
        "\n",
        "  # N_wind = number of samples / time bins for the R matrix = 3\n",
        "      # provided as an input\n",
        "\n",
        "  # samples (number of windows in the signal) = number of rows \n",
        "  samples = len(features)   # number of rows = number of windows\n",
        "\n",
        "  # ch = number of channels\n",
        "\n",
        "  # USED TO BE np.empty...\n",
        "  R = np.zeros((samples, (N_wind*len(features[0,:]))))  # len(features[0,:]) = (num of features)*(num of channels)\n",
        "                                                        # N_wind * len(features[0,:]) = (num of samples or time bins)*(num of features)*(num of channels)\n",
        "\n",
        "  f = 0\n",
        "  s = 1\n",
        "  t = 2\n",
        "\n",
        "  for i in range(len(features[0,:])):   # goes thru each column of the features matrix\n",
        "    # append every time bin (aka sample) for every column of the features matrix\n",
        "\n",
        "    R[:, f] = features_appended[0 : (len(features_appended)-2), i]\n",
        "    R[:, s] = features_appended[1 : (len(features_appended)-1), i]\n",
        "    R[:, t] = features_appended[2 : len(features_appended), i]\n",
        "\n",
        "    f += 3\n",
        "    s += 3\n",
        "    t += 3\n",
        "\n",
        "    #############\n",
        "\n",
        "    # R = np.append(R, features[0 : (len(features_appended)-2), i])  \n",
        "    # R = np.append(R, features[1 : (len(features_appended)-1), i])\n",
        "    # R = np.append(R, features[2 : len(features_appended), i]) \n",
        "\n",
        "  return R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BRc82EMdfux"
      },
      "source": [
        "# 4. ML Training and Testing (10 points)\n",
        "Here we will use the optimal linear decoder framework to predict finger angles, and additionally you will use one or more classifiers of your own choosing to make the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u79LdijvBgD"
      },
      "outputs": [],
      "source": [
        "#FOR ALL PATIENTS- NEED TO STANDARDIZE THE FEATURES\n",
        "\n",
        "def feat_standardize(feature_matrix,test_feature_matrix):\n",
        "\n",
        "  [windows,feats]=np.shape(feature_matrix)\n",
        "  [testwindows,feats_test]=np.shape(test_feature_matrix)\n",
        "  normFeats=np.empty((windows,feats))\n",
        "  testnormFeats=np.empty((testwindows,feats_test))\n",
        "\n",
        "  for nn in range(feats):\n",
        "    mean_train=np.mean(feature_matrix[:,nn])\n",
        "    std_train=np.std(feature_matrix[:,nn])\n",
        "    normFeats[:,nn]=(feature_matrix[:,nn]-mean_train)/std_train\n",
        "    mean_test=np.mean(test_feature_matrix[:,nn])\n",
        "    std_test=np.std(test_feature_matrix[:,nn])\n",
        "    testnormFeats[:,nn]=(test_feature_matrix[:,nn]-mean_train)/std_train #normalize test matrix by same mean/std as training \n",
        "\n",
        "  \n",
        "  return([normFeats,testnormFeats])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NHISkF2zfNT"
      },
      "outputs": [],
      "source": [
        "import sklearn as sk\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk2K_Of1uCBj"
      },
      "outputs": [],
      "source": [
        "def zoInterp(x, numInterp):\n",
        "  #return np.reshape(np.tile(x,numInterp),numInterp,'F')\n",
        "\n",
        "  repeat=np.tile(x[0],numInterp)\n",
        "  interpolation_x=repeat\n",
        "  for p in range(1,len(x)):\n",
        "    repeat=np.tile(x[p],numInterp)\n",
        "    interpolation_x=np.hstack((interpolation_x,repeat))\n",
        "  return interpolation_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFZE32rtuIh2"
      },
      "outputs": [],
      "source": [
        "window_length=100e-3\n",
        "window_overlap=50e-3\n",
        "fs=1000\n",
        "\n",
        "#window overlap in samples is amount to repeat each prediction \n",
        "winDisp_samples=round(window_overlap*fs) #window displacement in samples \n",
        "winLen_samples=round(window_length*fs) #window length in samples\n",
        "\n",
        "[M,rem_samples]=NumWins(testing_p1_glove[:,0],fs,window_length,window_overlap) #calculate number of windows and remaining time\n",
        "\n",
        "#remaining indices- need to pad\n",
        "rem_indices=rem_samples+winLen_samples-winDisp_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth(input, smoothSize):\n",
        "  #convThing = np.ones(smoothSize)/smoothSize;\n",
        "  convThing = sig.gaussian(smoothSize, 3)/smoothSize;\n",
        "  return np.convolve(input, convThing, mode='same')"
      ],
      "metadata": {
        "id": "sf393O90M2Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFtY-hu6bhY8"
      },
      "source": [
        "# Leaderboard Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOIGIm_7bqpc"
      },
      "outputs": [],
      "source": [
        "#Import leaderboard ecog data\n",
        "\n",
        "import scipy.io as sio\n",
        "leaderboard_data=sio.loadmat('leaderboard_data.mat')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEChTH1mc8jF"
      },
      "outputs": [],
      "source": [
        "leaderboard_data['leaderboard_ecog'][0]\n",
        "p1_board_ecog=leaderboard_data['leaderboard_ecog'][0][0]\n",
        "p2_board_ecog=leaderboard_data['leaderboard_ecog'][1][0]\n",
        "p3_board_ecog=leaderboard_data['leaderboard_ecog'][2][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KXGjyc2epiZ"
      },
      "outputs": [],
      "source": [
        "#check that the same channels are bad\n",
        "#plot all channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAqrOSYpdhkQ"
      },
      "outputs": [],
      "source": [
        "#pre-processing: removing bad channels\n",
        "\n",
        "p1_ecog=p1_board_ecog[:,np.arange(62)!=54] #remove channel 55\n",
        "p2_ecog=p2_board_ecog[:,np.logical_and(np.arange(48)!=20,np.arange(48)!=37)] #remove channel 21 and 38\n",
        "p3_ecog=p3_board_ecog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk0uiymWfCgD"
      },
      "outputs": [],
      "source": [
        "#filter, calculate features on the ecog\n",
        "\n",
        "window_length=100e-3\n",
        "window_overlap=50e-3\n",
        "fs=1000\n",
        "\n",
        "#calculate features for training data, call it p1_trainFeats\n",
        "p1_Feats_l=get_windowed_feats(p1_ecog, fs, window_length, window_overlap)\n",
        "p2_Feats_l=get_windowed_feats(p2_ecog, fs, window_length, window_overlap)\n",
        "p3_Feats_l=get_windowed_feats(p3_ecog, fs, window_length, window_overlap)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get entire raw ecog training data and glove data to train the model\n",
        "\n",
        "\n",
        "#Patient 1\n",
        "#glove\n",
        "total_p1_glove=proj_data['train_dg'][0][0]\n",
        "#ecog\n",
        "total_p1_ecog=proj_data['train_ecog'][0][0]\n",
        "raw_train_p1_ecog=total_p1_ecog[:,np.arange(62)!=54] \n",
        "\n",
        "#patient 2\n",
        "\n",
        "#glove\n",
        "total_p2_glove=proj_data['train_dg'][1][0]\n",
        "#ecog\n",
        "total_p2_ecog=proj_data['train_ecog'][1][0]\n",
        "raw_train_p2_ecog=total_p2_ecog[:,np.logical_and(np.arange(48)!=20,np.arange(48)!=37)] \n",
        "\n",
        "#patient 3\n",
        "\n",
        "#glove\n",
        "total_p3_glove=proj_data['train_dg'][2][0]\n",
        "#ecog\n",
        "total_p3_ecog=proj_data['train_ecog'][2][0]\n",
        "raw_train_p3_ecog=total_p3_ecog\n"
      ],
      "metadata": {
        "id": "Xtec_OeIimlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate new Y_train for each patient w the entire training data\n",
        "\n",
        "#calculate Y for the training data- basically flexion data downsampled to be Y=Mx5. ALL PATIENTS\n",
        "\n",
        "[M,rem_samples]=NumWins(total_p1_glove[:,0],fs,window_length,window_overlap) #calculate number of windows and remaining time\n",
        "\n",
        "#convert everything to units of samples\n",
        "winLen_samples=round(window_length*fs) #window length in samples\n",
        "winDisp_samples=round(window_overlap*fs) #window displacement in samples \n",
        "\n",
        "#starting index of x- first window occurs after remainder samples since right aligned\n",
        "window_start=rem_samples\n",
        "\n",
        "#initialize- patient 1\n",
        "flex_data=np.empty([int(M),5])\n",
        "\n",
        "for mm in range(round(M)): #for each window\n",
        "  flex_data[mm,:]=total_p1_glove[window_start,:] #find the flexion data for each finger at that window from glove data for P1\n",
        "  window_start=window_start+winDisp_samples #move to next window\n",
        "\n",
        "Y_train_1_all=flex_data\n",
        "print(np.shape(Y_train_1_all))\n",
        "\n",
        "#initialize- patient 2\n",
        "flex_data=np.empty([int(M),5])\n",
        "window_start=rem_samples\n",
        "for mm in range(round(M)): #for each window\n",
        "  flex_data[mm,:]=total_p2_glove[window_start,:] #find the flexion data for each finger at that window from glove data for P1\n",
        "  window_start=window_start+winDisp_samples #move to next window\n",
        "\n",
        "Y_train_2_all=flex_data\n",
        "print(np.shape(Y_train_2_all))\n",
        "\n",
        "#initialize- patient 3\n",
        "flex_data=np.empty([int(M),5])\n",
        "window_start=rem_samples\n",
        "for mm in range(round(M)): #for each window\n",
        "  flex_data[mm,:]=total_p3_glove[window_start,:] #find the flexion data for each finger at that window from glove data for P1\n",
        "  window_start=window_start+winDisp_samples #move to next window\n",
        "\n",
        "Y_train_3_all=flex_data\n",
        "print(np.shape(Y_train_3_all))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MPnkZjpkzr4",
        "outputId": "8ff6d1ca-87ce-4a1d-805c-7b36307fe8f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5999, 5)\n",
            "(5999, 5)\n",
            "(5999, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate features for the raw training data we will use to train model\n",
        "\n",
        "window_length=100e-3\n",
        "window_overlap=50e-3\n",
        "fs=1000\n",
        "\n",
        "#calculate features for training data, call it p1_trainFeats\n",
        "p1_alltrainFeats=get_windowed_feats(raw_train_p1_ecog, fs, window_length, window_overlap)\n",
        "p2_alltrainFeats=get_windowed_feats(raw_train_p2_ecog, fs, window_length, window_overlap)\n",
        "p3_alltrainFeats=get_windowed_feats(raw_train_p3_ecog, fs, window_length, window_overlap)\n"
      ],
      "metadata": {
        "id": "e7J3ByGjjlSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwbMrycsemEN"
      },
      "outputs": [],
      "source": [
        "#training and testing features normalize for all patients\n",
        "#use entire training data given\n",
        "[p1_FeatNorm_train,p1_FeatNorm_l]=feat_standardize(p1_alltrainFeats,p1_Feats_l)\n",
        "[p2_FeatNorm_train,p2_FeatNorm_l]=feat_standardize(p2_alltrainFeats,p2_Feats_l)\n",
        "[p3_FeatNorm_train,p3_FeatNorm_l]=feat_standardize(p3_alltrainFeats,p3_Feats_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJGs_KEGknhN"
      },
      "outputs": [],
      "source": [
        "#Calculate R matrix w 3 windows for each of the 3 patients normalized features for leaderboard\n",
        "R_P1_l=create_R_matrix(p1_FeatNorm_l,3) #N=3\n",
        "R_P2_l=create_R_matrix(p2_FeatNorm_l,3) #N=3\n",
        "R_P3_l=create_R_matrix(p3_FeatNorm_l,3) #N=3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate R matrix w 3 windows for each of 3 patients normalized features----- all training data\n",
        "\n",
        "R_P1_train=create_R_matrix(p1_FeatNorm_train,3) #N=3\n",
        "R_P2_train=create_R_matrix(p2_FeatNorm_train,3) #N=3\n",
        "R_P3_train=create_R_matrix(p3_FeatNorm_train,3) #N=3\n"
      ],
      "metadata": {
        "id": "KOzR53HgkWhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth(input, smoothSize):\n",
        "  #convThing = np.ones(smoothSize)/smoothSize;\n",
        "  convThing = sig.gaussian(smoothSize, 3)/smoothSize;\n",
        "  return np.convolve(input, convThing, mode='same')"
      ],
      "metadata": {
        "id": "dICaVn99NbUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "regressor = SVR(kernel = 'rbf')\n",
        "import sklearn as sk\n",
        "\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import RidgeCV, LassoCV,ElasticNetCV\n",
        "\n",
        "# elastic net:\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.datasets import make_regression"
      ],
      "metadata": {
        "id": "fj9eSH_IVPz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGEMu7mnlNX0"
      },
      "outputs": [],
      "source": [
        "#models and output predictions- elastic net\n",
        "\n",
        "samples,channels=np.shape(p1_board_ecog)\n",
        "\n",
        "#initialize arrays for glove prediction (num samples by 5 fingers)\n",
        "Patient_3_Prediction=np.zeros((samples,5))\n",
        "Patient_2_Prediction=np.zeros((samples,5))\n",
        "Patient_1_Prediction=np.zeros((samples,5))\n",
        "\n",
        "#for each finger\n",
        "for i in range(5):\n",
        "\n",
        "  #Patient 3\n",
        "  #fit model to the finger flexion values from training set \n",
        "\n",
        "  # now, create model according to optimum alpha value:\n",
        "  enet_cv_model3 = ElasticNetCV(cv = 10).fit(R_P3_train, Y_train_3_all[:,i])\n",
        "  enet_tuned3 = ElasticNet(alpha = enet_cv_model3.alpha_).fit(R_P3_train, Y_train_3_all[:,i])\n",
        "  # additionally, calculate error for (1) training set & (2) test set \n",
        "  y_pred_test3 = enet_tuned3.predict(R_P3_l)\n",
        "  #do smoothing\n",
        "  smoothed3=smooth(y_pred_test3,15)\n",
        "  #interpolate the prediction finger angles for this finger/patient\n",
        "  interpAngle3=zoInterp(smoothed3,winDisp_samples)\n",
        "  padding3=np.tile(interpAngle3[0],rem_indices)\n",
        "  totalPrediction3=np.transpose(np.hstack((padding3,interpAngle3)))\n",
        "  #add to prediction array\n",
        "  Patient_3_Prediction[:,i]=totalPrediction3\n",
        "\n",
        "  if i==0:\n",
        "    filename='p3_f1_model.sav'\n",
        "    pickle.dump(enet_tuned3, open(filename, 'wb'))\n",
        "  if i==1:\n",
        "    filename='p3_f2_model.sav'\n",
        "    pickle.dump(enet_tuned3, open(filename, 'wb'))\n",
        "  if i==2:\n",
        "    filename='p3_f3_model.sav'\n",
        "    pickle.dump(enet_tuned3, open(filename, 'wb'))\n",
        "  if i==3:\n",
        "    filename='p3_f4_model.sav'\n",
        "    pickle.dump(enet_tuned3, open(filename, 'wb'))\n",
        "  if i==4:\n",
        "    filename='p3_f5_model.sav'\n",
        "    pickle.dump(enet_tuned3, open(filename, 'wb'))\n",
        "\n",
        "  #Patient 2\n",
        "\n",
        "  # now, create model according to optimum alpha value:\n",
        "  enet_cv_model2 = ElasticNetCV(cv = 10).fit(R_P2_train, Y_train_2_all[:,i])\n",
        "  enet_tuned2 = ElasticNet(alpha = enet_cv_model2.alpha_).fit(R_P2_train, Y_train_2_all[:,i])\n",
        "  # additionally, calculate error for (1) training set & (2) test set \n",
        "  y_pred_test2 = enet_tuned2.predict(R_P2_l)\n",
        "  #do smoothing\n",
        "  smoothed2=smooth(y_pred_test2,15)\n",
        "\n",
        "  #interpolate the prediction finger angles for this finger/patient\n",
        "  interpAngle2=zoInterp(smoothed2,winDisp_samples)\n",
        "  padding2=np.tile(interpAngle2[0],rem_indices)\n",
        "  totalPrediction2=np.transpose(np.hstack((padding2,interpAngle2)))\n",
        "  #add to prediction array\n",
        "  Patient_2_Prediction[:,i]=totalPrediction2\n",
        "\n",
        "  if i==0:\n",
        "    filename='p2_f1_model.sav'\n",
        "    pickle.dump(enet_tuned2, open(filename, 'wb'))\n",
        "  if i==1:\n",
        "    filename='p2_f2_model.sav'\n",
        "    pickle.dump(enet_tuned2, open(filename, 'wb'))\n",
        "  if i==2:\n",
        "    filename='p2_f3_model.sav'\n",
        "    pickle.dump(enet_tuned2, open(filename, 'wb'))\n",
        "  if i==3:\n",
        "    filename='p2_f4_model.sav'\n",
        "    pickle.dump(enet_tuned2, open(filename, 'wb'))\n",
        "  if i==4:\n",
        "    filename='p2_f5_model.sav'\n",
        "    pickle.dump(enet_tuned2, open(filename, 'wb'))\n",
        "\n",
        "  #Patient 1\n",
        "\n",
        "  # now, create model according to optimum alpha value:\n",
        "  enet_cv_model1 = ElasticNetCV(cv = 10).fit(R_P1_train, Y_train_1_all[:,i])\n",
        "  enet_tuned1 = ElasticNet(alpha = enet_cv_model1.alpha_).fit(R_P1_train, Y_train_1_all[:,i])\n",
        "  # additionally, calculate error for (1) training set & (2) test set \n",
        "  y_pred_test1 = enet_tuned1.predict(R_P1_l)\n",
        "  #do smoothing\n",
        "  smoothed1=smooth(y_pred_test1,15)\n",
        "\n",
        "  #interpolate the prediction finger angles for this finger/patient\n",
        "  interpAngle1=zoInterp(smoothed1,winDisp_samples)\n",
        "  padding1=np.tile(interpAngle1[0],rem_indices)\n",
        "  totalPrediction1=np.transpose(np.hstack((padding1,interpAngle1)))\n",
        "  #add to prediction array\n",
        "  Patient_1_Prediction[:,i]=totalPrediction1\n",
        "\n",
        "  if i==0:\n",
        "    filename='p1_f1_model.sav'\n",
        "    pickle.dump(enet_tuned1, open(filename, 'wb'))\n",
        "  if i==1:\n",
        "    filename='p1_f2_model.sav'\n",
        "    pickle.dump(enet_tuned1, open(filename, 'wb'))\n",
        "  if i==2:\n",
        "    filename='p1_f3_model.sav'\n",
        "    pickle.dump(enet_tuned1, open(filename, 'wb'))\n",
        "  if i==3:\n",
        "    filename='p1_f4_model.sav'\n",
        "    pickle.dump(enet_tuned1, open(filename, 'wb'))\n",
        "  if i==4:\n",
        "    filename='p1_f5_model.sav'\n",
        "    pickle.dump(enet_tuned1, open(filename, 'wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9_xsPEr4ees",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63fe168b-bccb-4e76-9e7e-f0a4c948a8bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01035048,  0.01817266, -0.03639564, -0.09625615, -0.06482938],\n",
              "       [-0.01035048,  0.01817266, -0.03639564, -0.09625615, -0.06482938],\n",
              "       [-0.01035048,  0.01817266, -0.03639564, -0.09625615, -0.06482938],\n",
              "       ...,\n",
              "       [-0.1131203 ,  0.05634154,  0.02131697,  0.15211243,  0.05865611],\n",
              "       [-0.1131203 ,  0.05634154,  0.02131697,  0.15211243,  0.05865611],\n",
              "       [-0.1131203 ,  0.05634154,  0.02131697,  0.15211243,  0.05865611]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "Patient_1_Prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnnPredictions=np.load(\"RNNRes.npy\")"
      ],
      "metadata": {
        "id": "JnD386TkJnHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1_rnn=rnnPredictions[0]\n",
        "p2_rnn=rnnPredictions[1]\n",
        "p3_rnn=rnnPredictions[2]\n",
        "\n",
        "p1_f3_rnn=p1_rnn[:,2]\n",
        "p1_f5_rnn=p1_rnn[:,4]\n",
        "p2_f3_rnn=p2_rnn[:,2]\n",
        "p2_f5_rnn=p2_rnn[:,4]\n",
        "\n",
        "#do smoothing\n",
        "smoothedp1f3=smooth(p1_f3_rnn,15)\n",
        "smoothedp1f5=smooth(p1_f5_rnn,15)\n",
        "smoothedp2f3=smooth(p2_f3_rnn,15)\n",
        "smoothedp2f5=smooth(p2_f5_rnn,15)\n",
        "\n",
        "#interpolate the prediction finger angles for this finger/patient\n",
        "interpAngle1=zoInterp(smoothedp1f3,winDisp_samples)\n",
        "padding1=np.tile(interpAngle1[0],rem_indices)\n",
        "totalPrediction1=np.transpose(np.hstack((padding1,interpAngle1)))\n",
        "#add to prediction array\n",
        "Patient_1_Prediction[:,2]=totalPrediction1\n",
        "\n",
        "#interpolate the prediction finger angles for this finger/patient\n",
        "interpAngle1=zoInterp(smoothedp1f5,winDisp_samples)\n",
        "padding1=np.tile(interpAngle1[0],rem_indices)\n",
        "totalPrediction1=np.transpose(np.hstack((padding1,interpAngle1)))\n",
        "#add to prediction array\n",
        "Patient_1_Prediction[:,4]=totalPrediction1\n",
        "\n",
        "#interpolate the prediction finger angles for this finger/patient\n",
        "interpAngle1=zoInterp(smoothedp2f3,winDisp_samples)\n",
        "padding1=np.tile(interpAngle1[0],rem_indices)\n",
        "totalPrediction1=np.transpose(np.hstack((padding1,interpAngle1)))\n",
        "#add to prediction array\n",
        "Patient_2_Prediction[:,2]=totalPrediction1\n",
        "\n",
        "#interpolate the prediction finger angles for this finger/patient\n",
        "interpAngle1=zoInterp(smoothedp2f5,winDisp_samples)\n",
        "padding1=np.tile(interpAngle1[0],rem_indices)\n",
        "totalPrediction1=np.transpose(np.hstack((padding1,interpAngle1)))\n",
        "#add to prediction array\n",
        "Patient_2_Prediction[:,4]=totalPrediction1"
      ],
      "metadata": {
        "id": "WAOawOtW42WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Patient_1_Prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VrcOu0HG2RF",
        "outputId": "8226e7c9-9025-443c-a606-c68ce53ddd67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01035048,  0.01817266,  0.08795108, -0.09625615, -0.04243234],\n",
              "       [-0.01035048,  0.01817266,  0.08795108, -0.09625615, -0.04243234],\n",
              "       [-0.01035048,  0.01817266,  0.08795108, -0.09625615, -0.04243234],\n",
              "       ...,\n",
              "       [-0.1131203 ,  0.05634154, -0.10196276,  0.15211243,  0.07587013],\n",
              "       [-0.1131203 ,  0.05634154, -0.10196276,  0.15211243,  0.07587013],\n",
              "       [-0.1131203 ,  0.05634154, -0.10196276,  0.15211243,  0.07587013]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfqiTDQzxvB6"
      },
      "outputs": [],
      "source": [
        "#submit to leaderboard\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io import savemat\n",
        "#create an example submission array\n",
        "\n",
        "predictions = np.zeros((3,1), dtype=object)\n",
        "predictions[0,0] = Patient_1_Prediction\n",
        "predictions[1,0] = Patient_2_Prediction\n",
        "predictions[2,0] = Patient_3_Prediction\n",
        "\n",
        "#save the array using the right format\n",
        "savemat('predictions.mat', {'predicted_dg':predictions})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}